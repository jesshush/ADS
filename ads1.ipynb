{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Processing file: S008.txt\n",
      "📌 Initial number of rows: 418989\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.337773 -0.395162  0.288326  6\n",
      "1  2019-01-12 00:00:00.020 -0.984473 -0.021598  0.644522  6\n",
      "2  2019-01-12 00:00:00.040 -1.635857  0.230325 -0.192113  6\n",
      "3  2019-01-12 00:00:00.060 -0.363280  0.372247 -0.381429  6\n",
      "4  2019-01-12 00:00:00.080  0.008470  0.468494 -0.465681  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.337773 -0.395162  0.288326  6\n",
      "1  2019-01-12 00:00:00.020 -0.984473 -0.021598  0.644522  6\n",
      "2  2019-01-12 00:00:00.040 -1.635857  0.230325 -0.192113  6\n",
      "3  2019-01-12 00:00:00.060 -0.363280  0.372247 -0.381429  6\n",
      "4  2019-01-12 00:00:00.080  0.008470  0.468494 -0.465681  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 418989\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.337773 -0.395162  0.288326  6\n",
      "1  2019-01-12 00:00:00.020 -0.984473 -0.021598  0.644522  6\n",
      "2  2019-01-12 00:00:00.040 -1.635857  0.230325 -0.192113  6\n",
      "3  2019-01-12 00:00:00.060 -0.363280  0.372247 -0.381429  6\n",
      "4  2019-01-12 00:00:00.080  0.008470  0.468494 -0.465681  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.337773 -0.395162  0.288326  6\n",
      "1  2019-01-12 00:00:00.020 -0.984473 -0.021598  0.644522  6\n",
      "2  2019-01-12 00:00:00.040 -1.635857  0.230325 -0.192113  6\n",
      "3  2019-01-12 00:00:00.060 -0.363280  0.372247 -0.381429  6\n",
      "4  2019-01-12 00:00:00.080  0.008470  0.468494 -0.465681  6 \n",
      "\n",
      "🔍 Outliers removed: 61051\n",
      "✅ Rows after outlier removal: 357938\n",
      "📌 Data After Removing Outliers:\n",
      "                           0         1         2         3  4\n",
      "1   2019-01-12 00:00:00.020 -0.984473 -0.021598  0.644522  6\n",
      "7   2019-01-12 00:00:00.140 -1.379747  0.050676 -0.143763  6\n",
      "9   2019-01-12 00:00:00.180 -1.085551  0.103491 -0.147657  6\n",
      "10  2019-01-12 00:00:00.200 -0.890127  0.058093 -0.225672  6\n",
      "11  2019-01-12 00:00:00.220 -0.847437  0.207268 -0.194241  6 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 357938\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                           0         1         2         3  4\n",
      "1   2019-01-12 00:00:00.020 -0.984473 -0.021598  0.644522  6\n",
      "7   2019-01-12 00:00:00.140 -1.379747  0.050676 -0.143763  6\n",
      "9   2019-01-12 00:00:00.180 -1.085551  0.103491 -0.147657  6\n",
      "10  2019-01-12 00:00:00.200 -0.890127  0.058093 -0.225672  6\n",
      "11  2019-01-12 00:00:00.220 -0.847437  0.207268 -0.194241  6 \n",
      "\n",
      "📌 Final number of rows: 357938 (Rows Removed: 61051)\n",
      "✅ Cleaned data saved: Harth_Cleaned/S008.txt\n",
      "\n",
      "🔹 Processing file: S020.txt\n",
      "📌 Initial number of rows: 371496\n",
      "📌 Original Data Sample:\n",
      "                                0         1         2         3  4\n",
      "0  2016-12-05 13:17:56.548657565 -0.968183  0.031202 -0.144825  6\n",
      "1  2016-12-05 13:17:56.568657560 -1.040560 -0.028534 -0.137691  6\n",
      "2  2016-12-05 13:17:56.588657555 -0.987729 -0.054578 -0.098109  6\n",
      "3  2016-12-05 13:17:56.608657550 -1.034812 -0.110228 -0.095550  6\n",
      "4  2016-12-05 13:17:56.628657545 -0.958440 -0.055526 -0.116159  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                                0         1         2         3  4\n",
      "0  2016-12-05 13:17:56.548657565 -0.968183  0.031202 -0.144825  6\n",
      "1  2016-12-05 13:17:56.568657560 -1.040560 -0.028534 -0.137691  6\n",
      "2  2016-12-05 13:17:56.588657555 -0.987729 -0.054578 -0.098109  6\n",
      "3  2016-12-05 13:17:56.608657550 -1.034812 -0.110228 -0.095550  6\n",
      "4  2016-12-05 13:17:56.628657545 -0.958440 -0.055526 -0.116159  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 371496\n",
      "📌 Data After Removing Duplicates:\n",
      "                                0         1         2         3  4\n",
      "0  2016-12-05 13:17:56.548657565 -0.968183  0.031202 -0.144825  6\n",
      "1  2016-12-05 13:17:56.568657560 -1.040560 -0.028534 -0.137691  6\n",
      "2  2016-12-05 13:17:56.588657555 -0.987729 -0.054578 -0.098109  6\n",
      "3  2016-12-05 13:17:56.608657550 -1.034812 -0.110228 -0.095550  6\n",
      "4  2016-12-05 13:17:56.628657545 -0.958440 -0.055526 -0.116159  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                                0         1         2         3  4\n",
      "0  2016-12-05 13:17:56.548657565 -0.968183  0.031202 -0.144825  6\n",
      "1  2016-12-05 13:17:56.568657560 -1.040560 -0.028534 -0.137691  6\n",
      "2  2016-12-05 13:17:56.588657555 -0.987729 -0.054578 -0.098109  6\n",
      "3  2016-12-05 13:17:56.608657550 -1.034812 -0.110228 -0.095550  6\n",
      "4  2016-12-05 13:17:56.628657545 -0.958440 -0.055526 -0.116159  6 \n",
      "\n",
      "🔍 Outliers removed: 136353\n",
      "✅ Rows after outlier removal: 235143\n",
      "📌 Data After Removing Outliers:\n",
      "                                   0         1         2         3  4\n",
      "8468  2016-12-05 13:20:46.788615005 -0.492957  0.193861  0.347189  7\n",
      "8469  2016-12-05 13:20:46.808615000 -0.523328  0.142990  0.351687  7\n",
      "8470  2016-12-05 13:20:46.828614995 -0.532708  0.124340  0.359129  7\n",
      "8471  2016-12-05 13:20:46.848614990 -0.565149  0.120842  0.359229  7\n",
      "8472  2016-12-05 13:20:46.868614985 -0.594955  0.135688  0.352148  7 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 235143\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                                   0         1         2         3  4\n",
      "8468  2016-12-05 13:20:46.788615005 -0.492957  0.193861  0.347189  7\n",
      "8469  2016-12-05 13:20:46.808615000 -0.523328  0.142990  0.351687  7\n",
      "8470  2016-12-05 13:20:46.828614995 -0.532708  0.124340  0.359129  7\n",
      "8471  2016-12-05 13:20:46.848614990 -0.565149  0.120842  0.359229  7\n",
      "8472  2016-12-05 13:20:46.868614985 -0.594955  0.135688  0.352148  7 \n",
      "\n",
      "📌 Final number of rows: 235143 (Rows Removed: 136353)\n",
      "✅ Cleaned data saved: Harth_Cleaned/S020.txt\n",
      "\n",
      "🔹 Processing file: S021.txt\n",
      "📌 Initial number of rows: 302247\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.689075  0.386740  0.276759  6\n",
      "1  2019-01-12 00:00:00.020 -0.215083  0.002088 -0.394805  6\n",
      "2  2019-01-12 00:00:00.040 -0.006579 -0.768989 -0.708434  6\n",
      "3  2019-01-12 00:00:00.060 -0.201129 -1.340226 -0.618797  6\n",
      "4  2019-01-12 00:00:00.080 -0.914759 -0.654775  0.034583  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.689075  0.386740  0.276759  6\n",
      "1  2019-01-12 00:00:00.020 -0.215083  0.002088 -0.394805  6\n",
      "2  2019-01-12 00:00:00.040 -0.006579 -0.768989 -0.708434  6\n",
      "3  2019-01-12 00:00:00.060 -0.201129 -1.340226 -0.618797  6\n",
      "4  2019-01-12 00:00:00.080 -0.914759 -0.654775  0.034583  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 302247\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.689075  0.386740  0.276759  6\n",
      "1  2019-01-12 00:00:00.020 -0.215083  0.002088 -0.394805  6\n",
      "2  2019-01-12 00:00:00.040 -0.006579 -0.768989 -0.708434  6\n",
      "3  2019-01-12 00:00:00.060 -0.201129 -1.340226 -0.618797  6\n",
      "4  2019-01-12 00:00:00.080 -0.914759 -0.654775  0.034583  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.689075  0.386740  0.276759  6\n",
      "1  2019-01-12 00:00:00.020 -0.215083  0.002088 -0.394805  6\n",
      "2  2019-01-12 00:00:00.040 -0.006579 -0.768989 -0.708434  6\n",
      "3  2019-01-12 00:00:00.060 -0.201129 -1.340226 -0.618797  6\n",
      "4  2019-01-12 00:00:00.080 -0.914759 -0.654775  0.034583  6 \n",
      "\n",
      "🔍 Outliers removed: 70813\n",
      "✅ Rows after outlier removal: 231434\n",
      "📌 Data After Removing Outliers:\n",
      "                           0         1         2         3  4\n",
      "1   2019-01-12 00:00:00.020 -0.215083  0.002088 -0.394805  6\n",
      "5   2019-01-12 00:00:00.100 -0.962797  0.240441  0.016106  6\n",
      "14  2019-01-12 00:00:00.280 -0.919476  0.212522  0.001224  6\n",
      "16  2019-01-12 00:00:00.320 -0.846128  0.171009  0.060468  6\n",
      "17  2019-01-12 00:00:01.460 -0.990783  0.055901 -0.139445  6 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 231434\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                           0         1         2         3  4\n",
      "1   2019-01-12 00:00:00.020 -0.215083  0.002088 -0.394805  6\n",
      "5   2019-01-12 00:00:00.100 -0.962797  0.240441  0.016106  6\n",
      "14  2019-01-12 00:00:00.280 -0.919476  0.212522  0.001224  6\n",
      "16  2019-01-12 00:00:00.320 -0.846128  0.171009  0.060468  6\n",
      "17  2019-01-12 00:00:01.460 -0.990783  0.055901 -0.139445  6 \n",
      "\n",
      "📌 Final number of rows: 231434 (Rows Removed: 70813)\n",
      "✅ Cleaned data saved: Harth_Cleaned/S021.txt\n",
      "\n",
      "🔹 Processing file: S009.txt\n",
      "📌 Initial number of rows: 154464\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.990305 -0.083752 -0.061571  6\n",
      "1  2000-01-01 00:00:00.020 -0.992345 -0.088718 -0.060257  6\n",
      "2  2000-01-01 00:00:00.040 -0.970847 -0.089423 -0.071465  6\n",
      "3  2000-01-01 00:00:00.060 -1.007390 -0.076944 -0.066859  6\n",
      "4  2000-01-01 00:00:00.080 -0.981075 -0.072463 -0.063459  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.990305 -0.083752 -0.061571  6\n",
      "1  2000-01-01 00:00:00.020 -0.992345 -0.088718 -0.060257  6\n",
      "2  2000-01-01 00:00:00.040 -0.970847 -0.089423 -0.071465  6\n",
      "3  2000-01-01 00:00:00.060 -1.007390 -0.076944 -0.066859  6\n",
      "4  2000-01-01 00:00:00.080 -0.981075 -0.072463 -0.063459  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 154464\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.990305 -0.083752 -0.061571  6\n",
      "1  2000-01-01 00:00:00.020 -0.992345 -0.088718 -0.060257  6\n",
      "2  2000-01-01 00:00:00.040 -0.970847 -0.089423 -0.071465  6\n",
      "3  2000-01-01 00:00:00.060 -1.007390 -0.076944 -0.066859  6\n",
      "4  2000-01-01 00:00:00.080 -0.981075 -0.072463 -0.063459  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.990305 -0.083752 -0.061571  6\n",
      "1  2000-01-01 00:00:00.020 -0.992345 -0.088718 -0.060257  6\n",
      "2  2000-01-01 00:00:00.040 -0.970847 -0.089423 -0.071465  6\n",
      "3  2000-01-01 00:00:00.060 -1.007390 -0.076944 -0.066859  6\n",
      "4  2000-01-01 00:00:00.080 -0.981075 -0.072463 -0.063459  6 \n",
      "\n",
      "🔍 Outliers removed: 38139\n",
      "✅ Rows after outlier removal: 116325\n",
      "📌 Data After Removing Outliers:\n",
      "                             0         1         2         3   4\n",
      "3136  2000-01-01 00:01:04.360 -0.963548 -0.200084  0.067465  13\n",
      "3140  2000-01-01 00:01:04.440 -1.115532 -0.256584  0.341507  13\n",
      "3141  2000-01-01 00:01:04.460 -1.149611 -0.353316  0.323838  13\n",
      "3142  2000-01-01 00:01:04.480 -1.175511 -0.277809  0.271660  13\n",
      "3143  2000-01-01 00:01:04.500 -1.142267 -0.257200  0.280859  13 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 116325\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                             0         1         2         3   4\n",
      "3136  2000-01-01 00:01:04.360 -0.963548 -0.200084  0.067465  13\n",
      "3140  2000-01-01 00:01:04.440 -1.115532 -0.256584  0.341507  13\n",
      "3141  2000-01-01 00:01:04.460 -1.149611 -0.353316  0.323838  13\n",
      "3142  2000-01-01 00:01:04.480 -1.175511 -0.277809  0.271660  13\n",
      "3143  2000-01-01 00:01:04.500 -1.142267 -0.257200  0.280859  13 \n",
      "\n",
      "📌 Final number of rows: 116325 (Rows Removed: 38139)\n",
      "✅ Cleaned data saved: Harth_Cleaned/S009.txt\n",
      "\n",
      "🔹 Processing file: S019.txt\n",
      "📌 Initial number of rows: 297945\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.993507 -0.018688 -0.141448  6\n",
      "1  2019-01-12 00:00:00.020 -0.989250 -0.013345 -0.129213  6\n",
      "2  2019-01-12 00:00:00.040 -0.995267 -0.017828 -0.128375  6\n",
      "3  2019-01-12 00:00:00.060 -0.992888 -0.017299 -0.126825  6\n",
      "4  2019-01-12 00:00:00.080 -0.992648 -0.015724 -0.132142  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.993507 -0.018688 -0.141448  6\n",
      "1  2019-01-12 00:00:00.020 -0.989250 -0.013345 -0.129213  6\n",
      "2  2019-01-12 00:00:00.040 -0.995267 -0.017828 -0.128375  6\n",
      "3  2019-01-12 00:00:00.060 -0.992888 -0.017299 -0.126825  6\n",
      "4  2019-01-12 00:00:00.080 -0.992648 -0.015724 -0.132142  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 297945\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.993507 -0.018688 -0.141448  6\n",
      "1  2019-01-12 00:00:00.020 -0.989250 -0.013345 -0.129213  6\n",
      "2  2019-01-12 00:00:00.040 -0.995267 -0.017828 -0.128375  6\n",
      "3  2019-01-12 00:00:00.060 -0.992888 -0.017299 -0.126825  6\n",
      "4  2019-01-12 00:00:00.080 -0.992648 -0.015724 -0.132142  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.993507 -0.018688 -0.141448  6\n",
      "1  2019-01-12 00:00:00.020 -0.989250 -0.013345 -0.129213  6\n",
      "2  2019-01-12 00:00:00.040 -0.995267 -0.017828 -0.128375  6\n",
      "3  2019-01-12 00:00:00.060 -0.992888 -0.017299 -0.126825  6\n",
      "4  2019-01-12 00:00:00.080 -0.992648 -0.015724 -0.132142  6 \n",
      "\n",
      "🔍 Outliers removed: 120015\n",
      "✅ Rows after outlier removal: 177930\n",
      "📌 Data After Removing Outliers:\n",
      "                              0         1         2         3  4\n",
      "39744  2019-01-12 00:13:25.200 -0.733877 -0.060220  0.470609  7\n",
      "39745  2019-01-12 00:13:25.220 -0.648961 -0.036038  0.444768  7\n",
      "39746  2019-01-12 00:13:25.240 -0.612031  0.001122  0.431264  7\n",
      "39747  2019-01-12 00:13:25.260 -0.653599  0.018385  0.510136  7\n",
      "39748  2019-01-12 00:13:25.280 -0.666272  0.024684  0.456692  7 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 177930\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                              0         1         2         3  4\n",
      "39744  2019-01-12 00:13:25.200 -0.733877 -0.060220  0.470609  7\n",
      "39745  2019-01-12 00:13:25.220 -0.648961 -0.036038  0.444768  7\n",
      "39746  2019-01-12 00:13:25.240 -0.612031  0.001122  0.431264  7\n",
      "39747  2019-01-12 00:13:25.260 -0.653599  0.018385  0.510136  7\n",
      "39748  2019-01-12 00:13:25.280 -0.666272  0.024684  0.456692  7 \n",
      "\n",
      "📌 Final number of rows: 177930 (Rows Removed: 120015)\n",
      "❌ Error processing harth_txt_processed/S019.txt: [Errno 28] No space left on device\n",
      "\n",
      "🔹 Processing file: S015.txt\n",
      "📌 Initial number of rows: 418392\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.967728  0.023586 -0.165030  6\n",
      "1  2019-01-12 00:00:00.020 -0.962213  0.025477 -0.164427  6\n",
      "2  2019-01-12 00:00:00.040 -0.966207  0.015932 -0.160409  6\n",
      "3  2019-01-12 00:00:00.060 -0.966845  0.012804 -0.146368  6\n",
      "4  2019-01-12 00:00:00.080 -0.964110  0.013120 -0.161126  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.967728  0.023586 -0.165030  6\n",
      "1  2019-01-12 00:00:00.020 -0.962213  0.025477 -0.164427  6\n",
      "2  2019-01-12 00:00:00.040 -0.966207  0.015932 -0.160409  6\n",
      "3  2019-01-12 00:00:00.060 -0.966845  0.012804 -0.146368  6\n",
      "4  2019-01-12 00:00:00.080 -0.964110  0.013120 -0.161126  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 418392\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.967728  0.023586 -0.165030  6\n",
      "1  2019-01-12 00:00:00.020 -0.962213  0.025477 -0.164427  6\n",
      "2  2019-01-12 00:00:00.040 -0.966207  0.015932 -0.160409  6\n",
      "3  2019-01-12 00:00:00.060 -0.966845  0.012804 -0.146368  6\n",
      "4  2019-01-12 00:00:00.080 -0.964110  0.013120 -0.161126  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.967728  0.023586 -0.165030  6\n",
      "1  2019-01-12 00:00:00.020 -0.962213  0.025477 -0.164427  6\n",
      "2  2019-01-12 00:00:00.040 -0.966207  0.015932 -0.160409  6\n",
      "3  2019-01-12 00:00:00.060 -0.966845  0.012804 -0.146368  6\n",
      "4  2019-01-12 00:00:00.080 -0.964110  0.013120 -0.161126  6 \n",
      "\n",
      "🔍 Outliers removed: 137408\n",
      "✅ Rows after outlier removal: 280984\n",
      "📌 Data After Removing Outliers:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.967728  0.023586 -0.165030  6\n",
      "1  2019-01-12 00:00:00.020 -0.962213  0.025477 -0.164427  6\n",
      "2  2019-01-12 00:00:00.040 -0.966207  0.015932 -0.160409  6\n",
      "3  2019-01-12 00:00:00.060 -0.966845  0.012804 -0.146368  6\n",
      "4  2019-01-12 00:00:00.080 -0.964110  0.013120 -0.161126  6 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 280984\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.967728  0.023586 -0.165030  6\n",
      "1  2019-01-12 00:00:00.020 -0.962213  0.025477 -0.164427  6\n",
      "2  2019-01-12 00:00:00.040 -0.966207  0.015932 -0.160409  6\n",
      "3  2019-01-12 00:00:00.060 -0.966845  0.012804 -0.146368  6\n",
      "4  2019-01-12 00:00:00.080 -0.964110  0.013120 -0.161126  6 \n",
      "\n",
      "📌 Final number of rows: 280984 (Rows Removed: 137408)\n",
      "✅ Cleaned data saved: Harth_Cleaned/S015.txt\n",
      "\n",
      "🔹 Processing file: S014.txt\n",
      "📌 Initial number of rows: 366487\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.958333 -0.130516  0.030384  6\n",
      "1  2000-01-01 00:00:00.020 -1.009169 -0.101867  0.016758  6\n",
      "2  2000-01-01 00:00:00.040 -0.959140 -0.103409  0.012530  6\n",
      "3  2000-01-01 00:00:00.060 -1.003618 -0.105906  0.037492  6\n",
      "4  2000-01-01 00:00:00.080 -0.978285 -0.051315  0.005875  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.958333 -0.130516  0.030384  6\n",
      "1  2000-01-01 00:00:00.020 -1.009169 -0.101867  0.016758  6\n",
      "2  2000-01-01 00:00:00.040 -0.959140 -0.103409  0.012530  6\n",
      "3  2000-01-01 00:00:00.060 -1.003618 -0.105906  0.037492  6\n",
      "4  2000-01-01 00:00:00.080 -0.978285 -0.051315  0.005875  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 366487\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.958333 -0.130516  0.030384  6\n",
      "1  2000-01-01 00:00:00.020 -1.009169 -0.101867  0.016758  6\n",
      "2  2000-01-01 00:00:00.040 -0.959140 -0.103409  0.012530  6\n",
      "3  2000-01-01 00:00:00.060 -1.003618 -0.105906  0.037492  6\n",
      "4  2000-01-01 00:00:00.080 -0.978285 -0.051315  0.005875  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.958333 -0.130516  0.030384  6\n",
      "1  2000-01-01 00:00:00.020 -1.009169 -0.101867  0.016758  6\n",
      "2  2000-01-01 00:00:00.040 -0.959140 -0.103409  0.012530  6\n",
      "3  2000-01-01 00:00:00.060 -1.003618 -0.105906  0.037492  6\n",
      "4  2000-01-01 00:00:00.080 -0.978285 -0.051315  0.005875  6 \n",
      "\n",
      "🔍 Outliers removed: 67837\n",
      "✅ Rows after outlier removal: 298650\n",
      "📌 Data After Removing Outliers:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.958333 -0.130516  0.030384  6\n",
      "1  2000-01-01 00:00:00.020 -1.009169 -0.101867  0.016758  6\n",
      "2  2000-01-01 00:00:00.040 -0.959140 -0.103409  0.012530  6\n",
      "3  2000-01-01 00:00:00.060 -1.003618 -0.105906  0.037492  6\n",
      "4  2000-01-01 00:00:00.080 -0.978285 -0.051315  0.005875  6 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 298650\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.958333 -0.130516  0.030384  6\n",
      "1  2000-01-01 00:00:00.020 -1.009169 -0.101867  0.016758  6\n",
      "2  2000-01-01 00:00:00.040 -0.959140 -0.103409  0.012530  6\n",
      "3  2000-01-01 00:00:00.060 -1.003618 -0.105906  0.037492  6\n",
      "4  2000-01-01 00:00:00.080 -0.978285 -0.051315  0.005875  6 \n",
      "\n",
      "📌 Final number of rows: 298650 (Rows Removed: 67837)\n",
      "❌ Error processing harth_txt_processed/S014.txt: [Errno 28] No space left on device\n",
      "\n",
      "🔹 Processing file: S016.txt\n",
      "📌 Initial number of rows: 355418\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.958675 -0.012546 -0.090299  6\n",
      "1  2019-01-12 00:00:00.020 -0.954204 -0.001129 -0.087879  6\n",
      "2  2019-01-12 00:00:00.040 -0.958585 -0.002457 -0.092261  6\n",
      "3  2019-01-12 00:00:00.060 -0.957144  0.000777 -0.090378  6\n",
      "4  2019-01-12 00:00:00.080 -0.961486 -0.005533 -0.088443  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.958675 -0.012546 -0.090299  6\n",
      "1  2019-01-12 00:00:00.020 -0.954204 -0.001129 -0.087879  6\n",
      "2  2019-01-12 00:00:00.040 -0.958585 -0.002457 -0.092261  6\n",
      "3  2019-01-12 00:00:00.060 -0.957144  0.000777 -0.090378  6\n",
      "4  2019-01-12 00:00:00.080 -0.961486 -0.005533 -0.088443  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 355418\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.958675 -0.012546 -0.090299  6\n",
      "1  2019-01-12 00:00:00.020 -0.954204 -0.001129 -0.087879  6\n",
      "2  2019-01-12 00:00:00.040 -0.958585 -0.002457 -0.092261  6\n",
      "3  2019-01-12 00:00:00.060 -0.957144  0.000777 -0.090378  6\n",
      "4  2019-01-12 00:00:00.080 -0.961486 -0.005533 -0.088443  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.958675 -0.012546 -0.090299  6\n",
      "1  2019-01-12 00:00:00.020 -0.954204 -0.001129 -0.087879  6\n",
      "2  2019-01-12 00:00:00.040 -0.958585 -0.002457 -0.092261  6\n",
      "3  2019-01-12 00:00:00.060 -0.957144  0.000777 -0.090378  6\n",
      "4  2019-01-12 00:00:00.080 -0.961486 -0.005533 -0.088443  6 \n",
      "\n",
      "🔍 Outliers removed: 74104\n",
      "✅ Rows after outlier removal: 281314\n",
      "📌 Data After Removing Outliers:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.958675 -0.012546 -0.090299  6\n",
      "1  2019-01-12 00:00:00.020 -0.954204 -0.001129 -0.087879  6\n",
      "2  2019-01-12 00:00:00.040 -0.958585 -0.002457 -0.092261  6\n",
      "3  2019-01-12 00:00:00.060 -0.957144  0.000777 -0.090378  6\n",
      "4  2019-01-12 00:00:00.080 -0.961486 -0.005533 -0.088443  6 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 281314\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -0.958675 -0.012546 -0.090299  6\n",
      "1  2019-01-12 00:00:00.020 -0.954204 -0.001129 -0.087879  6\n",
      "2  2019-01-12 00:00:00.040 -0.958585 -0.002457 -0.092261  6\n",
      "3  2019-01-12 00:00:00.060 -0.957144  0.000777 -0.090378  6\n",
      "4  2019-01-12 00:00:00.080 -0.961486 -0.005533 -0.088443  6 \n",
      "\n",
      "📌 Final number of rows: 281314 (Rows Removed: 74104)\n",
      "❌ Error processing harth_txt_processed/S016.txt: [Errno 28] No space left on device: 'Harth_Cleaned/S016.txt'\n",
      "\n",
      "🔹 Processing file: S017.txt\n",
      "📌 Initial number of rows: 366609\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.966017  0.010835 -0.321281  6\n",
      "1  2000-01-01 00:00:00.020 -0.962151  0.014211 -0.340304  6\n",
      "2  2000-01-01 00:00:00.040 -0.962955  0.017351 -0.331718  6\n",
      "3  2000-01-01 00:00:00.060 -0.966502  0.015830 -0.335106  6\n",
      "4  2000-01-01 00:00:00.080 -0.955186  0.013582 -0.336803  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.966017  0.010835 -0.321281  6\n",
      "1  2000-01-01 00:00:00.020 -0.962151  0.014211 -0.340304  6\n",
      "2  2000-01-01 00:00:00.040 -0.962955  0.017351 -0.331718  6\n",
      "3  2000-01-01 00:00:00.060 -0.966502  0.015830 -0.335106  6\n",
      "4  2000-01-01 00:00:00.080 -0.955186  0.013582 -0.336803  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 366609\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.966017  0.010835 -0.321281  6\n",
      "1  2000-01-01 00:00:00.020 -0.962151  0.014211 -0.340304  6\n",
      "2  2000-01-01 00:00:00.040 -0.962955  0.017351 -0.331718  6\n",
      "3  2000-01-01 00:00:00.060 -0.966502  0.015830 -0.335106  6\n",
      "4  2000-01-01 00:00:00.080 -0.955186  0.013582 -0.336803  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.966017  0.010835 -0.321281  6\n",
      "1  2000-01-01 00:00:00.020 -0.962151  0.014211 -0.340304  6\n",
      "2  2000-01-01 00:00:00.040 -0.962955  0.017351 -0.331718  6\n",
      "3  2000-01-01 00:00:00.060 -0.966502  0.015830 -0.335106  6\n",
      "4  2000-01-01 00:00:00.080 -0.955186  0.013582 -0.336803  6 \n",
      "\n",
      "🔍 Outliers removed: 149453\n",
      "✅ Rows after outlier removal: 217156\n",
      "📌 Data After Removing Outliers:\n",
      "                             0         1         2         3  4\n",
      "6192  2000-01-01 00:02:19.840 -0.631123  0.095113  0.273095  7\n",
      "6193  2000-01-01 00:02:19.860 -0.591893  0.034040  0.249293  7\n",
      "6194  2000-01-01 00:02:19.880 -0.612461 -0.045407  0.236327  7\n",
      "6195  2000-01-01 00:02:19.900 -0.642447 -0.096115  0.284789  7\n",
      "6196  2000-01-01 00:02:19.920 -0.634241 -0.128013  0.319065  7 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 217156\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                             0         1         2         3  4\n",
      "6192  2000-01-01 00:02:19.840 -0.631123  0.095113  0.273095  7\n",
      "6193  2000-01-01 00:02:19.860 -0.591893  0.034040  0.249293  7\n",
      "6194  2000-01-01 00:02:19.880 -0.612461 -0.045407  0.236327  7\n",
      "6195  2000-01-01 00:02:19.900 -0.642447 -0.096115  0.284789  7\n",
      "6196  2000-01-01 00:02:19.920 -0.634241 -0.128013  0.319065  7 \n",
      "\n",
      "📌 Final number of rows: 217156 (Rows Removed: 149453)\n",
      "❌ Error processing harth_txt_processed/S017.txt: [Errno 28] No space left on device: 'Harth_Cleaned/S017.txt'\n",
      "\n",
      "🔹 Processing file: S013.txt\n",
      "📌 Initial number of rows: 369077\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.264407 -0.949509 -0.133860  6\n",
      "1  2000-01-01 00:00:00.020 -0.083919 -1.571325  0.357534  6\n",
      "2  2000-01-01 00:00:00.040 -0.639822 -0.968316 -1.016553  6\n",
      "3  2000-01-01 00:00:00.060 -0.597937 -0.450559 -2.071780  6\n",
      "4  2000-01-01 00:00:00.080 -1.363850  0.623318 -0.250756  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.264407 -0.949509 -0.133860  6\n",
      "1  2000-01-01 00:00:00.020 -0.083919 -1.571325  0.357534  6\n",
      "2  2000-01-01 00:00:00.040 -0.639822 -0.968316 -1.016553  6\n",
      "3  2000-01-01 00:00:00.060 -0.597937 -0.450559 -2.071780  6\n",
      "4  2000-01-01 00:00:00.080 -1.363850  0.623318 -0.250756  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 369077\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.264407 -0.949509 -0.133860  6\n",
      "1  2000-01-01 00:00:00.020 -0.083919 -1.571325  0.357534  6\n",
      "2  2000-01-01 00:00:00.040 -0.639822 -0.968316 -1.016553  6\n",
      "3  2000-01-01 00:00:00.060 -0.597937 -0.450559 -2.071780  6\n",
      "4  2000-01-01 00:00:00.080 -1.363850  0.623318 -0.250756  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -0.264407 -0.949509 -0.133860  6\n",
      "1  2000-01-01 00:00:00.020 -0.083919 -1.571325  0.357534  6\n",
      "2  2000-01-01 00:00:00.040 -0.639822 -0.968316 -1.016553  6\n",
      "3  2000-01-01 00:00:00.060 -0.597937 -0.450559 -2.071780  6\n",
      "4  2000-01-01 00:00:00.080 -1.363850  0.623318 -0.250756  6 \n",
      "\n",
      "🔍 Outliers removed: 89372\n",
      "✅ Rows after outlier removal: 279705\n",
      "📌 Data After Removing Outliers:\n",
      "                           0         1         2         3  4\n",
      "7   2000-01-01 00:00:00.140 -1.127030 -0.019938  0.129054  6\n",
      "8   2000-01-01 00:00:00.160 -1.066731 -0.148314  0.049698  6\n",
      "10  2000-01-01 00:00:00.200 -0.816655 -0.031345 -0.031935  6\n",
      "11  2000-01-01 00:00:00.220 -0.826869  0.063205 -0.089810  6\n",
      "12  2000-01-01 00:00:00.240 -0.955598  0.174361 -0.156389  6 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 279705\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                           0         1         2         3  4\n",
      "7   2000-01-01 00:00:00.140 -1.127030 -0.019938  0.129054  6\n",
      "8   2000-01-01 00:00:00.160 -1.066731 -0.148314  0.049698  6\n",
      "10  2000-01-01 00:00:00.200 -0.816655 -0.031345 -0.031935  6\n",
      "11  2000-01-01 00:00:00.220 -0.826869  0.063205 -0.089810  6\n",
      "12  2000-01-01 00:00:00.240 -0.955598  0.174361 -0.156389  6 \n",
      "\n",
      "📌 Final number of rows: 279705 (Rows Removed: 89372)\n",
      "❌ Error processing harth_txt_processed/S013.txt: [Errno 28] No space left on device: 'Harth_Cleaned/S013.txt'\n",
      "\n",
      "🔹 Processing file: S006.txt\n",
      "📌 Initial number of rows: 408709\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -2.926487  0.000463  0.589005  6\n",
      "1  2019-01-12 00:00:00.010  0.185204  0.284412  0.330148  6\n",
      "2  2019-01-12 00:00:00.020 -0.603182  0.053965 -0.341111  6\n",
      "3  2019-01-12 00:00:00.030 -1.101510 -0.467199 -0.137712  6\n",
      "4  2019-01-12 00:00:00.040 -0.451271  0.044536 -0.383600  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -2.926487  0.000463  0.589005  6\n",
      "1  2019-01-12 00:00:00.010  0.185204  0.284412  0.330148  6\n",
      "2  2019-01-12 00:00:00.020 -0.603182  0.053965 -0.341111  6\n",
      "3  2019-01-12 00:00:00.030 -1.101510 -0.467199 -0.137712  6\n",
      "4  2019-01-12 00:00:00.040 -0.451271  0.044536 -0.383600  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 408709\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -2.926487  0.000463  0.589005  6\n",
      "1  2019-01-12 00:00:00.010  0.185204  0.284412  0.330148  6\n",
      "2  2019-01-12 00:00:00.020 -0.603182  0.053965 -0.341111  6\n",
      "3  2019-01-12 00:00:00.030 -1.101510 -0.467199 -0.137712  6\n",
      "4  2019-01-12 00:00:00.040 -0.451271  0.044536 -0.383600  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -2.926487  0.000463  0.589005  6\n",
      "1  2019-01-12 00:00:00.010  0.185204  0.284412  0.330148  6\n",
      "2  2019-01-12 00:00:00.020 -0.603182  0.053965 -0.341111  6\n",
      "3  2019-01-12 00:00:00.030 -1.101510 -0.467199 -0.137712  6\n",
      "4  2019-01-12 00:00:00.040 -0.451271  0.044536 -0.383600  6 \n",
      "\n",
      "🔍 Outliers removed: 158272\n",
      "✅ Rows after outlier removal: 250437\n",
      "📌 Data After Removing Outliers:\n",
      "                              0         1         2         3  4\n",
      "24719  2019-01-12 00:04:17.110 -0.930025  0.158472  0.262419  7\n",
      "24720  2019-01-12 00:04:17.120 -0.953514  0.153717  0.227846  7\n",
      "24721  2019-01-12 00:04:17.130 -0.908761  0.152482  0.118452  7\n",
      "24722  2019-01-12 00:04:17.140 -0.918891  0.153448  0.093313  7\n",
      "24723  2019-01-12 00:04:17.150 -0.867056  0.152072  0.167604  7 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 250437\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                              0         1         2         3  4\n",
      "24719  2019-01-12 00:04:17.110 -0.930025  0.158472  0.262419  7\n",
      "24720  2019-01-12 00:04:17.120 -0.953514  0.153717  0.227846  7\n",
      "24721  2019-01-12 00:04:17.130 -0.908761  0.152482  0.118452  7\n",
      "24722  2019-01-12 00:04:17.140 -0.918891  0.153448  0.093313  7\n",
      "24723  2019-01-12 00:04:17.150 -0.867056  0.152072  0.167604  7 \n",
      "\n",
      "📌 Final number of rows: 250437 (Rows Removed: 158272)\n",
      "❌ Error processing harth_txt_processed/S006.txt: [Errno 28] No space left on device: 'Harth_Cleaned/S006.txt'\n",
      "\n",
      "🔹 Processing file: S012.txt\n",
      "📌 Initial number of rows: 382414\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -2.365008 -0.297340 -0.212054  6\n",
      "1  2000-01-01 00:00:00.020 -0.163226 -0.544052 -1.383367  6\n",
      "2  2000-01-01 00:00:00.040  0.643978 -0.316681 -1.478985  6\n",
      "3  2000-01-01 00:00:00.060 -0.079458 -1.406351 -0.301695  6\n",
      "4  2000-01-01 00:00:00.080 -2.132862  0.377162 -0.441734  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -2.365008 -0.297340 -0.212054  6\n",
      "1  2000-01-01 00:00:00.020 -0.163226 -0.544052 -1.383367  6\n",
      "2  2000-01-01 00:00:00.040  0.643978 -0.316681 -1.478985  6\n",
      "3  2000-01-01 00:00:00.060 -0.079458 -1.406351 -0.301695  6\n",
      "4  2000-01-01 00:00:00.080 -2.132862  0.377162 -0.441734  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 382414\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -2.365008 -0.297340 -0.212054  6\n",
      "1  2000-01-01 00:00:00.020 -0.163226 -0.544052 -1.383367  6\n",
      "2  2000-01-01 00:00:00.040  0.643978 -0.316681 -1.478985  6\n",
      "3  2000-01-01 00:00:00.060 -0.079458 -1.406351 -0.301695  6\n",
      "4  2000-01-01 00:00:00.080 -2.132862  0.377162 -0.441734  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2000-01-01 00:00:00.000 -2.365008 -0.297340 -0.212054  6\n",
      "1  2000-01-01 00:00:00.020 -0.163226 -0.544052 -1.383367  6\n",
      "2  2000-01-01 00:00:00.040  0.643978 -0.316681 -1.478985  6\n",
      "3  2000-01-01 00:00:00.060 -0.079458 -1.406351 -0.301695  6\n",
      "4  2000-01-01 00:00:00.080 -2.132862  0.377162 -0.441734  6 \n",
      "\n",
      "🔍 Outliers removed: 114371\n",
      "✅ Rows after outlier removal: 268043\n",
      "📌 Data After Removing Outliers:\n",
      "                            0         1         2         3  4\n",
      "147  2000-01-01 00:00:05.060 -0.859479  0.033163  0.444375  7\n",
      "148  2000-01-01 00:00:05.080 -0.751884  0.012497  0.327728  7\n",
      "149  2000-01-01 00:00:05.100 -0.696875  0.000625  0.367822  7\n",
      "150  2000-01-01 00:00:05.120 -0.658482 -0.048533  0.426531  7\n",
      "151  2000-01-01 00:00:05.140 -0.608809 -0.089378  0.384727  7 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 268043\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                            0         1         2         3  4\n",
      "147  2000-01-01 00:00:05.060 -0.859479  0.033163  0.444375  7\n",
      "148  2000-01-01 00:00:05.080 -0.751884  0.012497  0.327728  7\n",
      "149  2000-01-01 00:00:05.100 -0.696875  0.000625  0.367822  7\n",
      "150  2000-01-01 00:00:05.120 -0.658482 -0.048533  0.426531  7\n",
      "151  2000-01-01 00:00:05.140 -0.608809 -0.089378  0.384727  7 \n",
      "\n",
      "📌 Final number of rows: 268043 (Rows Removed: 114371)\n",
      "❌ Error processing harth_txt_processed/S012.txt: [Errno 28] No space left on device: 'Harth_Cleaned/S012.txt'\n",
      "\n",
      "🔹 Processing file: S010.txt\n",
      "📌 Initial number of rows: 351649\n",
      "📌 Original Data Sample:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.057197 -0.084939  0.490602  6\n",
      "1  2019-01-12 00:00:00.020 -0.867287  0.362947 -0.288787  6\n",
      "2  2019-01-12 00:00:00.040 -0.654156  0.153709 -0.090835  6\n",
      "3  2019-01-12 00:00:00.060 -0.493392 -0.278395 -0.067897  6\n",
      "4  2019-01-12 00:00:00.080 -0.685662 -0.016548 -0.147109  6 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.057197 -0.084939  0.490602  6\n",
      "1  2019-01-12 00:00:00.020 -0.867287  0.362947 -0.288787  6\n",
      "2  2019-01-12 00:00:00.040 -0.654156  0.153709 -0.090835  6\n",
      "3  2019-01-12 00:00:00.060 -0.493392 -0.278395 -0.067897  6\n",
      "4  2019-01-12 00:00:00.080 -0.685662 -0.016548 -0.147109  6 \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 351649\n",
      "📌 Data After Removing Duplicates:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.057197 -0.084939  0.490602  6\n",
      "1  2019-01-12 00:00:00.020 -0.867287  0.362947 -0.288787  6\n",
      "2  2019-01-12 00:00:00.040 -0.654156  0.153709 -0.090835  6\n",
      "3  2019-01-12 00:00:00.060 -0.493392 -0.278395 -0.067897  6\n",
      "4  2019-01-12 00:00:00.080 -0.685662 -0.016548 -0.147109  6 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "                          0         1         2         3  4\n",
      "0  2019-01-12 00:00:00.000 -1.057197 -0.084939  0.490602  6\n",
      "1  2019-01-12 00:00:00.020 -0.867287  0.362947 -0.288787  6\n",
      "2  2019-01-12 00:00:00.040 -0.654156  0.153709 -0.090835  6\n",
      "3  2019-01-12 00:00:00.060 -0.493392 -0.278395 -0.067897  6\n",
      "4  2019-01-12 00:00:00.080 -0.685662 -0.016548 -0.147109  6 \n",
      "\n",
      "🔍 Outliers removed: 128331\n",
      "✅ Rows after outlier removal: 223318\n",
      "📌 Data After Removing Outliers:\n",
      "                          0         1         2         3  4\n",
      "2  2019-01-12 00:00:00.040 -0.654156  0.153709 -0.090835  6\n",
      "4  2019-01-12 00:00:00.080 -0.685662 -0.016548 -0.147109  6\n",
      "6  2019-01-12 00:00:00.120 -0.920817 -0.145920 -0.045650  6\n",
      "7  2019-01-12 00:00:00.140 -1.137585  0.124606  0.006412  6\n",
      "9  2019-01-12 00:00:00.180 -0.983939  0.149527  0.041028  6 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 223318\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "                          0         1         2         3  4\n",
      "2  2019-01-12 00:00:00.040 -0.654156  0.153709 -0.090835  6\n",
      "4  2019-01-12 00:00:00.080 -0.685662 -0.016548 -0.147109  6\n",
      "6  2019-01-12 00:00:00.120 -0.920817 -0.145920 -0.045650  6\n",
      "7  2019-01-12 00:00:00.140 -1.137585  0.124606  0.006412  6\n",
      "9  2019-01-12 00:00:00.180 -0.983939  0.149527  0.041028  6 \n",
      "\n",
      "📌 Final number of rows: 223318 (Rows Removed: 128331)\n",
      "❌ Error processing harth_txt_processed/S010.txt: [Errno 28] No space left on device\n",
      "\n",
      "✅ Data cleaning completed for all Harth processed files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define directories\n",
    "input_dir = \"harth_txt_processed\"  # Main dataset folder\n",
    "cleaned_dir = \"Harth_Cleaned\"  # Folder for cleaned files\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "# Function to clean each dataset\n",
    "def clean_dataset(file_path, save_path):\n",
    "    try:\n",
    "        # Read TXT file as DataFrame (assuming space-separated values)\n",
    "        df = pd.read_csv(file_path, sep=\" \", header=None, engine=\"python\")\n",
    "\n",
    "        print(f\"\\n🔹 Processing file: {os.path.basename(file_path)}\")\n",
    "\n",
    "        # Initial number of rows\n",
    "        initial_rows = df.shape[0]\n",
    "        print(f\"📌 Initial number of rows: {initial_rows}\")\n",
    "        print(\"📌 Original Data Sample:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 1️⃣ Handling Missing Values ###\n",
    "        missing_values_before = df.isnull().sum().sum()\n",
    "        print(f\"🔍 Missing values before: {missing_values_before}\")\n",
    "\n",
    "        # Fill numerical columns with median, categorical with mode\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'O':  # Categorical\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            else:  # Numerical\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "        missing_values_after = df.isnull().sum().sum()\n",
    "        print(f\"✅ Missing values after: {missing_values_after}\")\n",
    "        print(\"📌 Data After Handling Missing Values:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 2️⃣ Removing Redundant Values (Duplicates) ###\n",
    "        duplicate_rows = df.duplicated().sum()\n",
    "        print(f\"🔍 Duplicate rows found: {duplicate_rows}\")\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        after_duplicate_rows = df.shape[0]\n",
    "        print(f\"✅ Rows after duplicate removal: {after_duplicate_rows}\")\n",
    "        print(\"📌 Data After Removing Duplicates:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 3️⃣ Fixing Structural Errors (Trimming & Lowercasing) ###\n",
    "        df = df.applymap(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "        print(\"📌 Data After Fixing Structural Errors:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 4️⃣ Handling Outliers (IQR Method) ###\n",
    "        outliers_removed = 0\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            before_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
    "            outliers_removed += before_outliers\n",
    "            \n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "        after_outliers_rows = df.shape[0]\n",
    "        print(f\"🔍 Outliers removed: {outliers_removed}\")\n",
    "        print(f\"✅ Rows after outlier removal: {after_outliers_rows}\")\n",
    "        print(\"📌 Data After Removing Outliers:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 5️⃣ Data Imputation (Re-check for missing values) ###\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                if df[col].dtype == 'O':\n",
    "                    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "                else:\n",
    "                    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "        ### 6️⃣ Removing Unwanted Observations ###\n",
    "        before_removal = df.shape[0]\n",
    "        df = df[~df.apply(lambda row: row.astype(str).str.contains(\"unknown|error\", case=False).any(), axis=1)]\n",
    "        removed_obs = before_removal - df.shape[0]\n",
    "        after_removal_rows = df.shape[0]\n",
    "        print(f\"🔍 Unwanted observations removed: {removed_obs}\")\n",
    "        print(f\"✅ Rows after removing unwanted observations: {after_removal_rows}\")\n",
    "        print(\"📌 Data After Removing Unwanted Observations:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        # Final row count\n",
    "        final_rows = df.shape[0]\n",
    "        print(f\"📌 Final number of rows: {final_rows} (Rows Removed: {initial_rows - final_rows})\")\n",
    "\n",
    "        # Save cleaned data\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_csv(save_path, sep=\" \", index=False, header=False)\n",
    "\n",
    "        print(f\"✅ Cleaned data saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "\n",
    "# Traverse harth_txt_processed directories and clean all text files\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            input_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(input_path, input_dir)  # Preserve directory structure\n",
    "            output_path = os.path.join(cleaned_dir, relative_path)\n",
    "            clean_dataset(input_path, output_path)\n",
    "\n",
    "print(\"\\n✅ Data cleaning completed for all Harth processed files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Processing file: features.txt\n",
      "📌 Initial number of rows: 561\n",
      "📌 Original Data Sample:\n",
      "    0                  1\n",
      "0  1  tBodyAcc-mean()-X\n",
      "1  2  tBodyAcc-mean()-Y\n",
      "2  3  tBodyAcc-mean()-Z\n",
      "3  4   tBodyAcc-std()-X\n",
      "4  5   tBodyAcc-std()-Y \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "    0                  1\n",
      "0  1  tBodyAcc-mean()-X\n",
      "1  2  tBodyAcc-mean()-Y\n",
      "2  3  tBodyAcc-mean()-Z\n",
      "3  4   tBodyAcc-std()-X\n",
      "4  5   tBodyAcc-std()-Y \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 561\n",
      "📌 Data After Removing Duplicates:\n",
      "    0                  1\n",
      "0  1  tBodyAcc-mean()-X\n",
      "1  2  tBodyAcc-mean()-Y\n",
      "2  3  tBodyAcc-mean()-Z\n",
      "3  4   tBodyAcc-std()-X\n",
      "4  5   tBodyAcc-std()-Y \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "    0                  1\n",
      "0  1  tbodyacc-mean()-x\n",
      "1  2  tbodyacc-mean()-y\n",
      "2  3  tbodyacc-mean()-z\n",
      "3  4   tbodyacc-std()-x\n",
      "4  5   tbodyacc-std()-y \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 561\n",
      "📌 Data After Removing Outliers:\n",
      "    0                  1\n",
      "0  1  tbodyacc-mean()-x\n",
      "1  2  tbodyacc-mean()-y\n",
      "2  3  tbodyacc-mean()-z\n",
      "3  4   tbodyacc-std()-x\n",
      "4  5   tbodyacc-std()-y \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 561\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "    0                  1\n",
      "0  1  tbodyacc-mean()-x\n",
      "1  2  tbodyacc-mean()-y\n",
      "2  3  tbodyacc-mean()-z\n",
      "3  4   tbodyacc-std()-x\n",
      "4  5   tbodyacc-std()-y \n",
      "\n",
      "📌 Final number of rows: 561 (Rows Removed: 0)\n",
      "✅ Cleaned data saved: Harus_Cleaned/features.txt\n",
      "\n",
      "🔹 Processing file: activity_labels.txt\n",
      "📌 Initial number of rows: 6\n",
      "📌 Original Data Sample:\n",
      "    0                   1\n",
      "0  1             WALKING\n",
      "1  2    WALKING_UPSTAIRS\n",
      "2  3  WALKING_DOWNSTAIRS\n",
      "3  4             SITTING\n",
      "4  5            STANDING \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "    0                   1\n",
      "0  1             WALKING\n",
      "1  2    WALKING_UPSTAIRS\n",
      "2  3  WALKING_DOWNSTAIRS\n",
      "3  4             SITTING\n",
      "4  5            STANDING \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 6\n",
      "📌 Data After Removing Duplicates:\n",
      "    0                   1\n",
      "0  1             WALKING\n",
      "1  2    WALKING_UPSTAIRS\n",
      "2  3  WALKING_DOWNSTAIRS\n",
      "3  4             SITTING\n",
      "4  5            STANDING \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "    0                   1\n",
      "0  1             walking\n",
      "1  2    walking_upstairs\n",
      "2  3  walking_downstairs\n",
      "3  4             sitting\n",
      "4  5            standing \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 6\n",
      "📌 Data After Removing Outliers:\n",
      "    0                   1\n",
      "0  1             walking\n",
      "1  2    walking_upstairs\n",
      "2  3  walking_downstairs\n",
      "3  4             sitting\n",
      "4  5            standing \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 6\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "    0                   1\n",
      "0  1             walking\n",
      "1  2    walking_upstairs\n",
      "2  3  walking_downstairs\n",
      "3  4             sitting\n",
      "4  5            standing \n",
      "\n",
      "📌 Final number of rows: 6 (Rows Removed: 0)\n",
      "✅ Cleaned data saved: Harus_Cleaned/activity_labels.txt\n",
      "❌ Error processing Harus/features_info.txt: Expected 3 fields in line 13, saw 92\n",
      "❌ Error processing Harus/README.txt: 'utf-8' codec can't decode byte 0xe0 in position 441: invalid continuation byte\n",
      "\n",
      "🔹 Processing file: subject_test.txt\n",
      "📌 Initial number of rows: 2947\n",
      "📌 Original Data Sample:\n",
      "    0\n",
      "0  2\n",
      "1  2\n",
      "2  2\n",
      "3  2\n",
      "4  2 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "    0\n",
      "0  2\n",
      "1  2\n",
      "2  2\n",
      "3  2\n",
      "4  2 \n",
      "\n",
      "🔍 Duplicate rows found: 2938\n",
      "✅ Rows after duplicate removal: 9\n",
      "📌 Data After Removing Duplicates:\n",
      "        0\n",
      "0      2\n",
      "302    4\n",
      "619    9\n",
      "907   10\n",
      "1201  12 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "        0\n",
      "0      2\n",
      "302    4\n",
      "619    9\n",
      "907   10\n",
      "1201  12 \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 9\n",
      "📌 Data After Removing Outliers:\n",
      "        0\n",
      "0      2\n",
      "302    4\n",
      "619    9\n",
      "907   10\n",
      "1201  12 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 9\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "        0\n",
      "0      2\n",
      "302    4\n",
      "619    9\n",
      "907   10\n",
      "1201  12 \n",
      "\n",
      "📌 Final number of rows: 9 (Rows Removed: 2938)\n",
      "✅ Cleaned data saved: Harus_Cleaned/test/subject_test.txt\n",
      "❌ Error processing Harus/test/X_test.txt: Expected 667 fields in line 22, saw 674\n",
      "\n",
      "🔹 Processing file: y_test.txt\n",
      "📌 Initial number of rows: 2947\n",
      "📌 Original Data Sample:\n",
      "    0\n",
      "0  5\n",
      "1  5\n",
      "2  5\n",
      "3  5\n",
      "4  5 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "    0\n",
      "0  5\n",
      "1  5\n",
      "2  5\n",
      "3  5\n",
      "4  5 \n",
      "\n",
      "🔍 Duplicate rows found: 2941\n",
      "✅ Rows after duplicate removal: 6\n",
      "📌 Data After Removing Duplicates:\n",
      "      0\n",
      "0    5\n",
      "31   4\n",
      "55   6\n",
      "79   1\n",
      "109  3 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "      0\n",
      "0    5\n",
      "31   4\n",
      "55   6\n",
      "79   1\n",
      "109  3 \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 6\n",
      "📌 Data After Removing Outliers:\n",
      "      0\n",
      "0    5\n",
      "31   4\n",
      "55   6\n",
      "79   1\n",
      "109  3 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 6\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "      0\n",
      "0    5\n",
      "31   4\n",
      "55   6\n",
      "79   1\n",
      "109  3 \n",
      "\n",
      "📌 Final number of rows: 6 (Rows Removed: 2941)\n",
      "✅ Cleaned data saved: Harus_Cleaned/test/y_test.txt\n",
      "❌ Error processing Harus/test/Inertial Signals/total_acc_y_test.txt: Expected 129 fields in line 56, saw 257\n",
      "\n",
      "🔹 Processing file: total_acc_x_test.txt\n",
      "📌 Initial number of rows: 2947\n",
      "📌 Original Data Sample:\n",
      "    0    1         2    3         4    5         6    7         8    9    ...  \\\n",
      "0  NaN  NaN  1.041216  NaN  1.041803  NaN  1.039086  NaN  1.054768  NaN  ...   \n",
      "1  NaN  NaN  0.999192  NaN  0.994679  NaN  0.993552  NaN  0.998674  NaN  ...   \n",
      "2  NaN  NaN  0.997593  NaN  0.998970  NaN  0.997057  NaN  0.994424  NaN  ...   \n",
      "3  NaN  NaN  0.994564  NaN  0.994727  NaN  0.991648  NaN  0.989580  NaN  ...   \n",
      "4  NaN  NaN  0.988150  NaN  0.989711  NaN  0.987535  NaN  0.987721  NaN  ...   \n",
      "\n",
      "   247       248  249       250  251       252  253       254  255       256  \n",
      "0  NaN  0.994884  NaN  0.992978  NaN  0.993016  NaN  0.993241  NaN  0.994391  \n",
      "1  NaN  0.989947  NaN  0.996258  NaN  1.001861  NaN  0.997521  NaN  0.992861  \n",
      "2  NaN  0.993750  NaN  0.993438  NaN  0.991880  NaN  0.990663  NaN  0.988245  \n",
      "3  NaN  0.987672  NaN  0.989598  NaN  0.991496  NaN  0.990630  NaN  0.990583  \n",
      "4  NaN  0.996847  NaN  0.994718  NaN  0.992234  NaN  0.989202  NaN  0.988737  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Missing values before: 380163\n",
      "✅ Missing values after: 2947\n",
      "📌 Data After Handling Missing Values:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.065083  1.041216 -0.063911  1.041803 -0.063586  1.039086 -0.062633   \n",
      "1  NaN -0.065083  0.999192 -0.063911  0.994679 -0.063586  0.993552 -0.062633   \n",
      "2  NaN -0.065083  0.997593 -0.063911  0.998970 -0.063586  0.997057 -0.062633   \n",
      "3  NaN -0.065083  0.994564 -0.063911  0.994727 -0.063586  0.991648 -0.062633   \n",
      "4  NaN -0.065083  0.988150 -0.063911  0.989711 -0.063586  0.987535 -0.062633   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  1.054768 -0.064889  ...  0.026382  0.994884  0.028991  0.992978  0.040803   \n",
      "1  0.998674 -0.064889  ...  0.026382  0.989947  0.028991  0.996258  0.040803   \n",
      "2  0.994424 -0.064889  ...  0.026382  0.993750  0.028991  0.993438  0.040803   \n",
      "3  0.989580 -0.064889  ...  0.026382  0.987672  0.028991  0.989598  0.040803   \n",
      "4  0.987721 -0.064889  ...  0.026382  0.996847  0.028991  0.994718  0.040803   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.993016  0.055296  0.993241  0.053941  0.994391  \n",
      "1  1.001861  0.055296  0.997521  0.053941  0.992861  \n",
      "2  0.991880  0.055296  0.990663  0.053941  0.988245  \n",
      "3  0.991496  0.055296  0.990630  0.053941  0.990583  \n",
      "4  0.992234  0.055296  0.989202  0.053941  0.988737  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 2947\n",
      "📌 Data After Removing Duplicates:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.065083  1.041216 -0.063911  1.041803 -0.063586  1.039086 -0.062633   \n",
      "1  NaN -0.065083  0.999192 -0.063911  0.994679 -0.063586  0.993552 -0.062633   \n",
      "2  NaN -0.065083  0.997593 -0.063911  0.998970 -0.063586  0.997057 -0.062633   \n",
      "3  NaN -0.065083  0.994564 -0.063911  0.994727 -0.063586  0.991648 -0.062633   \n",
      "4  NaN -0.065083  0.988150 -0.063911  0.989711 -0.063586  0.987535 -0.062633   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  1.054768 -0.064889  ...  0.026382  0.994884  0.028991  0.992978  0.040803   \n",
      "1  0.998674 -0.064889  ...  0.026382  0.989947  0.028991  0.996258  0.040803   \n",
      "2  0.994424 -0.064889  ...  0.026382  0.993750  0.028991  0.993438  0.040803   \n",
      "3  0.989580 -0.064889  ...  0.026382  0.987672  0.028991  0.989598  0.040803   \n",
      "4  0.987721 -0.064889  ...  0.026382  0.996847  0.028991  0.994718  0.040803   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.993016  0.055296  0.993241  0.053941  0.994391  \n",
      "1  1.001861  0.055296  0.997521  0.053941  0.992861  \n",
      "2  0.991880  0.055296  0.990663  0.053941  0.988245  \n",
      "3  0.991496  0.055296  0.990630  0.053941  0.990583  \n",
      "4  0.992234  0.055296  0.989202  0.053941  0.988737  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.065083  1.041216 -0.063911  1.041803 -0.063586  1.039086 -0.062633   \n",
      "1  NaN -0.065083  0.999192 -0.063911  0.994679 -0.063586  0.993552 -0.062633   \n",
      "2  NaN -0.065083  0.997593 -0.063911  0.998970 -0.063586  0.997057 -0.062633   \n",
      "3  NaN -0.065083  0.994564 -0.063911  0.994727 -0.063586  0.991648 -0.062633   \n",
      "4  NaN -0.065083  0.988150 -0.063911  0.989711 -0.063586  0.987535 -0.062633   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  1.054768 -0.064889  ...  0.026382  0.994884  0.028991  0.992978  0.040803   \n",
      "1  0.998674 -0.064889  ...  0.026382  0.989947  0.028991  0.996258  0.040803   \n",
      "2  0.994424 -0.064889  ...  0.026382  0.993750  0.028991  0.993438  0.040803   \n",
      "3  0.989580 -0.064889  ...  0.026382  0.987672  0.028991  0.989598  0.040803   \n",
      "4  0.987721 -0.064889  ...  0.026382  0.996847  0.028991  0.994718  0.040803   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.993016  0.055296  0.993241  0.053941  0.994391  \n",
      "1  1.001861  0.055296  0.997521  0.053941  0.992861  \n",
      "2  0.991880  0.055296  0.990663  0.053941  0.988245  \n",
      "3  0.991496  0.055296  0.990630  0.053941  0.990583  \n",
      "4  0.992234  0.055296  0.989202  0.053941  0.988737  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 0\n",
      "📌 Data After Removing Outliers:\n",
      " Empty DataFrame\n",
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 257 columns] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tfod/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 0\n",
      "📌 Data After Removing Unwanted Observations:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      "📌 Final number of rows: 0 (Rows Removed: 2947)\n",
      "✅ Cleaned data saved: Harus_Cleaned/test/Inertial Signals/total_acc_x_test.txt\n",
      "❌ Error processing Harus/test/Inertial Signals/body_acc_x_test.txt: Expected 193 fields in line 2, saw 215\n",
      "❌ Error processing Harus/test/Inertial Signals/body_acc_y_test.txt: Expected 197 fields in line 2, saw 212\n",
      "❌ Error processing Harus/test/Inertial Signals/body_gyro_y_test.txt: Expected 169 fields in line 5, saw 183\n",
      "❌ Error processing Harus/test/Inertial Signals/body_gyro_x_test.txt: Expected 254 fields in line 67, saw 257\n",
      "\n",
      "🔹 Processing file: total_acc_z_test.txt\n",
      "📌 Initial number of rows: 2947\n",
      "📌 Original Data Sample:\n",
      "    0    1         2    3         4    5         6    7         8    9    ...  \\\n",
      "0  NaN  NaN  0.023780  NaN  0.076293  NaN  0.147475  NaN  0.139906  NaN  ...   \n",
      "1  NaN  NaN  0.125616  NaN  0.125625  NaN  0.116381  NaN  0.107567  NaN  ...   \n",
      "2  NaN  NaN  0.150774  NaN  0.153943  NaN  0.144154  NaN  0.132821  NaN  ...   \n",
      "3  NaN  NaN  0.111199  NaN  0.115503  NaN  0.135251  NaN  0.160527  NaN  ...   \n",
      "4  NaN  NaN  0.133833  NaN  0.138512  NaN  0.139431  NaN  0.137851  NaN  ...   \n",
      "\n",
      "   247       248  249       250  251       252  253       254  255       256  \n",
      "0  NaN  0.145261  NaN  0.143904  NaN  0.144395  NaN  0.144703  NaN  0.145494  \n",
      "1  NaN  0.147877  NaN  0.153025  NaN  0.152788  NaN  0.139843  NaN  0.121314  \n",
      "2  NaN  0.133174  NaN  0.133712  NaN  0.132678  NaN  0.132694  NaN  0.132117  \n",
      "3  NaN  0.117611  NaN  0.118865  NaN  0.116060  NaN  0.110997  NaN  0.111912  \n",
      "4  NaN  0.125458  NaN  0.129365  NaN  0.125895  NaN  0.122510  NaN  0.122760  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Missing values before: 380163\n",
      "✅ Missing values after: 2947\n",
      "📌 Data After Handling Missing Values:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.127302  0.023780 -0.122611  0.076293 -0.116196  0.147475 -0.111839   \n",
      "1  NaN -0.127302  0.125616 -0.122611  0.125625 -0.116196  0.116381 -0.111839   \n",
      "2  NaN -0.127302  0.150774 -0.122611  0.153943 -0.116196  0.144154 -0.111839   \n",
      "3  NaN -0.127302  0.111199 -0.122611  0.115503 -0.116196  0.135251 -0.111839   \n",
      "4  NaN -0.127302  0.133833 -0.122611  0.138512 -0.116196  0.139431 -0.111839   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  0.139906 -0.114284  ...  0.093839  0.145261  0.088549  0.143904  0.099965   \n",
      "1  0.107567 -0.114284  ...  0.093839  0.147877  0.088549  0.153025  0.099965   \n",
      "2  0.132821 -0.114284  ...  0.093839  0.133174  0.088549  0.133712  0.099965   \n",
      "3  0.160527 -0.114284  ...  0.093839  0.117611  0.088549  0.118865  0.099965   \n",
      "4  0.137851 -0.114284  ...  0.093839  0.125458  0.088549  0.129365  0.099965   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.144395  0.106296  0.144703  0.095285  0.145494  \n",
      "1  0.152788  0.106296  0.139843  0.095285  0.121314  \n",
      "2  0.132678  0.106296  0.132694  0.095285  0.132117  \n",
      "3  0.116060  0.106296  0.110997  0.095285  0.111912  \n",
      "4  0.125895  0.106296  0.122510  0.095285  0.122760  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 2947\n",
      "📌 Data After Removing Duplicates:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.127302  0.023780 -0.122611  0.076293 -0.116196  0.147475 -0.111839   \n",
      "1  NaN -0.127302  0.125616 -0.122611  0.125625 -0.116196  0.116381 -0.111839   \n",
      "2  NaN -0.127302  0.150774 -0.122611  0.153943 -0.116196  0.144154 -0.111839   \n",
      "3  NaN -0.127302  0.111199 -0.122611  0.115503 -0.116196  0.135251 -0.111839   \n",
      "4  NaN -0.127302  0.133833 -0.122611  0.138512 -0.116196  0.139431 -0.111839   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  0.139906 -0.114284  ...  0.093839  0.145261  0.088549  0.143904  0.099965   \n",
      "1  0.107567 -0.114284  ...  0.093839  0.147877  0.088549  0.153025  0.099965   \n",
      "2  0.132821 -0.114284  ...  0.093839  0.133174  0.088549  0.133712  0.099965   \n",
      "3  0.160527 -0.114284  ...  0.093839  0.117611  0.088549  0.118865  0.099965   \n",
      "4  0.137851 -0.114284  ...  0.093839  0.125458  0.088549  0.129365  0.099965   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.144395  0.106296  0.144703  0.095285  0.145494  \n",
      "1  0.152788  0.106296  0.139843  0.095285  0.121314  \n",
      "2  0.132678  0.106296  0.132694  0.095285  0.132117  \n",
      "3  0.116060  0.106296  0.110997  0.095285  0.111912  \n",
      "4  0.125895  0.106296  0.122510  0.095285  0.122760  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.127302  0.023780 -0.122611  0.076293 -0.116196  0.147475 -0.111839   \n",
      "1  NaN -0.127302  0.125616 -0.122611  0.125625 -0.116196  0.116381 -0.111839   \n",
      "2  NaN -0.127302  0.150774 -0.122611  0.153943 -0.116196  0.144154 -0.111839   \n",
      "3  NaN -0.127302  0.111199 -0.122611  0.115503 -0.116196  0.135251 -0.111839   \n",
      "4  NaN -0.127302  0.133833 -0.122611  0.138512 -0.116196  0.139431 -0.111839   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  0.139906 -0.114284  ...  0.093839  0.145261  0.088549  0.143904  0.099965   \n",
      "1  0.107567 -0.114284  ...  0.093839  0.147877  0.088549  0.153025  0.099965   \n",
      "2  0.132821 -0.114284  ...  0.093839  0.133174  0.088549  0.133712  0.099965   \n",
      "3  0.160527 -0.114284  ...  0.093839  0.117611  0.088549  0.118865  0.099965   \n",
      "4  0.137851 -0.114284  ...  0.093839  0.125458  0.088549  0.129365  0.099965   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.144395  0.106296  0.144703  0.095285  0.145494  \n",
      "1  0.152788  0.106296  0.139843  0.095285  0.121314  \n",
      "2  0.132678  0.106296  0.132694  0.095285  0.132117  \n",
      "3  0.116060  0.106296  0.110997  0.095285  0.111912  \n",
      "4  0.125895  0.106296  0.122510  0.095285  0.122760  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 0\n",
      "📌 Data After Removing Outliers:\n",
      " Empty DataFrame\n",
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 257 columns] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tfod/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 0\n",
      "📌 Data After Removing Unwanted Observations:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      "📌 Final number of rows: 0 (Rows Removed: 2947)\n",
      "✅ Cleaned data saved: Harus_Cleaned/test/Inertial Signals/total_acc_z_test.txt\n",
      "❌ Error processing Harus/test/Inertial Signals/body_gyro_z_test.txt: Expected 250 fields in line 2, saw 257\n",
      "❌ Error processing Harus/test/Inertial Signals/body_acc_z_test.txt: Expected 223 fields in line 11, saw 224\n",
      "❌ Error processing Harus/train/X_train.txt: Expected 662 fields in line 27, saw 665\n",
      "\n",
      "🔹 Processing file: y_train.txt\n",
      "📌 Initial number of rows: 7352\n",
      "📌 Original Data Sample:\n",
      "    0\n",
      "0  5\n",
      "1  5\n",
      "2  5\n",
      "3  5\n",
      "4  5 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "    0\n",
      "0  5\n",
      "1  5\n",
      "2  5\n",
      "3  5\n",
      "4  5 \n",
      "\n",
      "🔍 Duplicate rows found: 7346\n",
      "✅ Rows after duplicate removal: 6\n",
      "📌 Data After Removing Duplicates:\n",
      "      0\n",
      "0    5\n",
      "27   4\n",
      "51   6\n",
      "78   1\n",
      "125  3 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "      0\n",
      "0    5\n",
      "27   4\n",
      "51   6\n",
      "78   1\n",
      "125  3 \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 6\n",
      "📌 Data After Removing Outliers:\n",
      "      0\n",
      "0    5\n",
      "27   4\n",
      "51   6\n",
      "78   1\n",
      "125  3 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 6\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "      0\n",
      "0    5\n",
      "27   4\n",
      "51   6\n",
      "78   1\n",
      "125  3 \n",
      "\n",
      "📌 Final number of rows: 6 (Rows Removed: 7346)\n",
      "✅ Cleaned data saved: Harus_Cleaned/train/y_train.txt\n",
      "\n",
      "🔹 Processing file: subject_train.txt\n",
      "📌 Initial number of rows: 7352\n",
      "📌 Original Data Sample:\n",
      "    0\n",
      "0  1\n",
      "1  1\n",
      "2  1\n",
      "3  1\n",
      "4  1 \n",
      "\n",
      "🔍 Missing values before: 0\n",
      "✅ Missing values after: 0\n",
      "📌 Data After Handling Missing Values:\n",
      "    0\n",
      "0  1\n",
      "1  1\n",
      "2  1\n",
      "3  1\n",
      "4  1 \n",
      "\n",
      "🔍 Duplicate rows found: 7331\n",
      "✅ Rows after duplicate removal: 21\n",
      "📌 Data After Removing Duplicates:\n",
      "       0\n",
      "0     1\n",
      "347   3\n",
      "688   5\n",
      "990   6\n",
      "1315  7 \n",
      "\n",
      "📌 Data After Fixing Structural Errors:\n",
      "       0\n",
      "0     1\n",
      "347   3\n",
      "688   5\n",
      "990   6\n",
      "1315  7 \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 21\n",
      "📌 Data After Removing Outliers:\n",
      "       0\n",
      "0     1\n",
      "347   3\n",
      "688   5\n",
      "990   6\n",
      "1315  7 \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 21\n",
      "📌 Data After Removing Unwanted Observations:\n",
      "       0\n",
      "0     1\n",
      "347   3\n",
      "688   5\n",
      "990   6\n",
      "1315  7 \n",
      "\n",
      "📌 Final number of rows: 21 (Rows Removed: 7331)\n",
      "✅ Cleaned data saved: Harus_Cleaned/train/subject_train.txt\n",
      "❌ Error processing Harus/train/Inertial Signals/body_gyro_z_train.txt: Expected 248 fields in line 19, saw 257\n",
      "❌ Error processing Harus/train/Inertial Signals/body_acc_y_train.txt: Expected 167 fields in line 2, saw 189\n",
      "\n",
      "🔹 Processing file: total_acc_z_train.txt\n",
      "📌 Initial number of rows: 7352\n",
      "📌 Original Data Sample:\n",
      "    0    1         2    3         4    5         6    7         8    9    ...  \\\n",
      "0  NaN  NaN  0.102934  NaN  0.105687  NaN  0.102102  NaN  0.106553  NaN  ...   \n",
      "1  NaN  NaN  0.097930  NaN  0.099351  NaN  0.098114  NaN  0.097517  NaN  ...   \n",
      "2  NaN  NaN  0.091117  NaN  0.092676  NaN  0.096064  NaN  0.099897  NaN  ...   \n",
      "3  NaN  NaN  0.095152  NaN  0.095415  NaN  0.088274  NaN  0.086325  NaN  ...   \n",
      "4  NaN  NaN  0.080841  NaN  0.079127  NaN  0.078291  NaN  0.084063  NaN  ...   \n",
      "\n",
      "   247       248  249       250  251       252  253       254  255       256  \n",
      "0  NaN  0.094843  NaN  0.098350  NaN  0.100385  NaN  0.099874  NaN  0.094987  \n",
      "1  NaN  0.095126  NaN  0.099496  NaN  0.093535  NaN  0.089035  NaN  0.090612  \n",
      "2  NaN  0.081413  NaN  0.081936  NaN  0.083011  NaN  0.082334  NaN  0.081487  \n",
      "3  NaN  0.082785  NaN  0.084084  NaN  0.085761  NaN  0.083275  NaN  0.081404  \n",
      "4  NaN  0.081640  NaN  0.079652  NaN  0.081329  NaN  0.085397  NaN  0.088816  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Missing values before: 948408\n",
      "✅ Missing values after: 7352\n",
      "📌 Data After Handling Missing Values:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.162636  0.102934 -0.155417  0.105687 -0.147383  0.102102 -0.149747   \n",
      "1  NaN -0.162636  0.097930 -0.155417  0.099351 -0.147383  0.098114 -0.149747   \n",
      "2  NaN -0.162636  0.091117 -0.155417  0.092676 -0.147383  0.096064 -0.149747   \n",
      "3  NaN -0.162636  0.095152 -0.155417  0.095415 -0.147383  0.088274 -0.149747   \n",
      "4  NaN -0.162636  0.080841 -0.155417  0.079127 -0.147383  0.078291 -0.149747   \n",
      "\n",
      "        8         9    ...       247       248       249       250      251  \\\n",
      "0  0.106553 -0.143537  ...  0.095676  0.094843  0.099422  0.098350  0.12094   \n",
      "1  0.097517 -0.143537  ...  0.095676  0.095126  0.099422  0.099496  0.12094   \n",
      "2  0.099897 -0.143537  ...  0.095676  0.081413  0.099422  0.081936  0.12094   \n",
      "3  0.086325 -0.143537  ...  0.095676  0.082785  0.099422  0.084084  0.12094   \n",
      "4  0.084063 -0.143537  ...  0.095676  0.081640  0.099422  0.079652  0.12094   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.100385  0.132688  0.099874  0.136444  0.094987  \n",
      "1  0.093535  0.132688  0.089035  0.136444  0.090612  \n",
      "2  0.083011  0.132688  0.082334  0.136444  0.081487  \n",
      "3  0.085761  0.132688  0.083275  0.136444  0.081404  \n",
      "4  0.081329  0.132688  0.085397  0.136444  0.088816  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 7352\n",
      "📌 Data After Removing Duplicates:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.162636  0.102934 -0.155417  0.105687 -0.147383  0.102102 -0.149747   \n",
      "1  NaN -0.162636  0.097930 -0.155417  0.099351 -0.147383  0.098114 -0.149747   \n",
      "2  NaN -0.162636  0.091117 -0.155417  0.092676 -0.147383  0.096064 -0.149747   \n",
      "3  NaN -0.162636  0.095152 -0.155417  0.095415 -0.147383  0.088274 -0.149747   \n",
      "4  NaN -0.162636  0.080841 -0.155417  0.079127 -0.147383  0.078291 -0.149747   \n",
      "\n",
      "        8         9    ...       247       248       249       250      251  \\\n",
      "0  0.106553 -0.143537  ...  0.095676  0.094843  0.099422  0.098350  0.12094   \n",
      "1  0.097517 -0.143537  ...  0.095676  0.095126  0.099422  0.099496  0.12094   \n",
      "2  0.099897 -0.143537  ...  0.095676  0.081413  0.099422  0.081936  0.12094   \n",
      "3  0.086325 -0.143537  ...  0.095676  0.082785  0.099422  0.084084  0.12094   \n",
      "4  0.084063 -0.143537  ...  0.095676  0.081640  0.099422  0.079652  0.12094   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.100385  0.132688  0.099874  0.136444  0.094987  \n",
      "1  0.093535  0.132688  0.089035  0.136444  0.090612  \n",
      "2  0.083011  0.132688  0.082334  0.136444  0.081487  \n",
      "3  0.085761  0.132688  0.083275  0.136444  0.081404  \n",
      "4  0.081329  0.132688  0.085397  0.136444  0.088816  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tfod/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Data After Fixing Structural Errors:\n",
      "    0         1         2         3         4         5         6         7    \\\n",
      "0  NaN -0.162636  0.102934 -0.155417  0.105687 -0.147383  0.102102 -0.149747   \n",
      "1  NaN -0.162636  0.097930 -0.155417  0.099351 -0.147383  0.098114 -0.149747   \n",
      "2  NaN -0.162636  0.091117 -0.155417  0.092676 -0.147383  0.096064 -0.149747   \n",
      "3  NaN -0.162636  0.095152 -0.155417  0.095415 -0.147383  0.088274 -0.149747   \n",
      "4  NaN -0.162636  0.080841 -0.155417  0.079127 -0.147383  0.078291 -0.149747   \n",
      "\n",
      "        8         9    ...       247       248       249       250      251  \\\n",
      "0  0.106553 -0.143537  ...  0.095676  0.094843  0.099422  0.098350  0.12094   \n",
      "1  0.097517 -0.143537  ...  0.095676  0.095126  0.099422  0.099496  0.12094   \n",
      "2  0.099897 -0.143537  ...  0.095676  0.081413  0.099422  0.081936  0.12094   \n",
      "3  0.086325 -0.143537  ...  0.095676  0.082785  0.099422  0.084084  0.12094   \n",
      "4  0.084063 -0.143537  ...  0.095676  0.081640  0.099422  0.079652  0.12094   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  0.100385  0.132688  0.099874  0.136444  0.094987  \n",
      "1  0.093535  0.132688  0.089035  0.136444  0.090612  \n",
      "2  0.083011  0.132688  0.082334  0.136444  0.081487  \n",
      "3  0.085761  0.132688  0.083275  0.136444  0.081404  \n",
      "4  0.081329  0.132688  0.085397  0.136444  0.088816  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 0\n",
      "📌 Data After Removing Outliers:\n",
      " Empty DataFrame\n",
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 257 columns] \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 0\n",
      "📌 Data After Removing Unwanted Observations:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      "📌 Final number of rows: 0 (Rows Removed: 7352)\n",
      "✅ Cleaned data saved: Harus_Cleaned/train/Inertial Signals/total_acc_z_train.txt\n",
      "❌ Error processing Harus/train/Inertial Signals/body_acc_x_train.txt: Expected 219 fields in line 15, saw 227\n",
      "❌ Error processing Harus/train/Inertial Signals/body_gyro_x_train.txt: Expected 254 fields in line 44, saw 257\n",
      "❌ Error processing Harus/train/Inertial Signals/total_acc_y_train.txt: Expected 129 fields in line 28, saw 257\n",
      "❌ Error processing Harus/train/Inertial Signals/body_gyro_y_train.txt: Expected 238 fields in line 15, saw 257\n",
      "\n",
      "🔹 Processing file: total_acc_x_train.txt\n",
      "📌 Initial number of rows: 7352\n",
      "📌 Original Data Sample:\n",
      "    0    1         2    3         4    5         6    7         8    9    ...  \\\n",
      "0  NaN  NaN  1.012817  NaN  1.022833  NaN  1.022028  NaN  1.017877  NaN  ...   \n",
      "1  NaN  NaN  1.018851  NaN  1.022380  NaN  1.020781  NaN  1.020218  NaN  ...   \n",
      "2  NaN  NaN  1.023127  NaN  1.021882  NaN  1.019178  NaN  1.015861  NaN  ...   \n",
      "3  NaN  NaN  1.017682  NaN  1.018149  NaN  1.019854  NaN  1.019880  NaN  ...   \n",
      "4  NaN  NaN  1.019952  NaN  1.019616  NaN  1.020933  NaN  1.023061  NaN  ...   \n",
      "\n",
      "   247       248  249       250  251       252  253       254  255       256  \n",
      "0  NaN  1.019815  NaN  1.019290  NaN  1.018445  NaN  1.019372  NaN  1.021171  \n",
      "1  NaN  1.018685  NaN  1.015660  NaN  1.014788  NaN  1.016499  NaN  1.017849  \n",
      "2  NaN  1.019434  NaN  1.019916  NaN  1.021041  NaN  1.022935  NaN  1.022019  \n",
      "3  NaN  1.018887  NaN  1.019161  NaN  1.019916  NaN  1.019602  NaN  1.020735  \n",
      "4  NaN  1.023884  NaN  1.021753  NaN  1.019425  NaN  1.018896  NaN  1.016787  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Missing values before: 948408\n",
      "✅ Missing values after: 7352\n",
      "📌 Data After Handling Missing Values:\n",
      "    0         1         2        3         4         5         6        7    \\\n",
      "0  NaN -0.065913  1.012817 -0.06698  1.022833 -0.065939  1.022028 -0.06372   \n",
      "1  NaN -0.065913  1.018851 -0.06698  1.022380 -0.065939  1.020781 -0.06372   \n",
      "2  NaN -0.065913  1.023127 -0.06698  1.021882 -0.065939  1.019178 -0.06372   \n",
      "3  NaN -0.065913  1.017682 -0.06698  1.018149 -0.065939  1.019854 -0.06372   \n",
      "4  NaN -0.065913  1.019952 -0.06698  1.019616 -0.065939  1.020933 -0.06372   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  1.017877 -0.065087  ...  0.020638  1.019815  0.022129  1.019290  0.019855   \n",
      "1  1.020218 -0.065087  ...  0.020638  1.018685  0.022129  1.015660  0.019855   \n",
      "2  1.015861 -0.065087  ...  0.020638  1.019434  0.022129  1.019916  0.019855   \n",
      "3  1.019880 -0.065087  ...  0.020638  1.018887  0.022129  1.019161  0.019855   \n",
      "4  1.023061 -0.065087  ...  0.020638  1.023884  0.022129  1.021753  0.019855   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  1.018445  0.027825  1.019372  0.025792  1.021171  \n",
      "1  1.014788  0.027825  1.016499  0.025792  1.017849  \n",
      "2  1.021041  0.027825  1.022935  0.025792  1.022019  \n",
      "3  1.019916  0.027825  1.019602  0.025792  1.020735  \n",
      "4  1.019425  0.027825  1.018896  0.025792  1.016787  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Duplicate rows found: 0\n",
      "✅ Rows after duplicate removal: 7352\n",
      "📌 Data After Removing Duplicates:\n",
      "    0         1         2        3         4         5         6        7    \\\n",
      "0  NaN -0.065913  1.012817 -0.06698  1.022833 -0.065939  1.022028 -0.06372   \n",
      "1  NaN -0.065913  1.018851 -0.06698  1.022380 -0.065939  1.020781 -0.06372   \n",
      "2  NaN -0.065913  1.023127 -0.06698  1.021882 -0.065939  1.019178 -0.06372   \n",
      "3  NaN -0.065913  1.017682 -0.06698  1.018149 -0.065939  1.019854 -0.06372   \n",
      "4  NaN -0.065913  1.019952 -0.06698  1.019616 -0.065939  1.020933 -0.06372   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  1.017877 -0.065087  ...  0.020638  1.019815  0.022129  1.019290  0.019855   \n",
      "1  1.020218 -0.065087  ...  0.020638  1.018685  0.022129  1.015660  0.019855   \n",
      "2  1.015861 -0.065087  ...  0.020638  1.019434  0.022129  1.019916  0.019855   \n",
      "3  1.019880 -0.065087  ...  0.020638  1.018887  0.022129  1.019161  0.019855   \n",
      "4  1.023061 -0.065087  ...  0.020638  1.023884  0.022129  1.021753  0.019855   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  1.018445  0.027825  1.019372  0.025792  1.021171  \n",
      "1  1.014788  0.027825  1.016499  0.025792  1.017849  \n",
      "2  1.021041  0.027825  1.022935  0.025792  1.022019  \n",
      "3  1.019916  0.027825  1.019602  0.025792  1.020735  \n",
      "4  1.019425  0.027825  1.018896  0.025792  1.016787  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tfod/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Data After Fixing Structural Errors:\n",
      "    0         1         2        3         4         5         6        7    \\\n",
      "0  NaN -0.065913  1.012817 -0.06698  1.022833 -0.065939  1.022028 -0.06372   \n",
      "1  NaN -0.065913  1.018851 -0.06698  1.022380 -0.065939  1.020781 -0.06372   \n",
      "2  NaN -0.065913  1.023127 -0.06698  1.021882 -0.065939  1.019178 -0.06372   \n",
      "3  NaN -0.065913  1.017682 -0.06698  1.018149 -0.065939  1.019854 -0.06372   \n",
      "4  NaN -0.065913  1.019952 -0.06698  1.019616 -0.065939  1.020933 -0.06372   \n",
      "\n",
      "        8         9    ...       247       248       249       250       251  \\\n",
      "0  1.017877 -0.065087  ...  0.020638  1.019815  0.022129  1.019290  0.019855   \n",
      "1  1.020218 -0.065087  ...  0.020638  1.018685  0.022129  1.015660  0.019855   \n",
      "2  1.015861 -0.065087  ...  0.020638  1.019434  0.022129  1.019916  0.019855   \n",
      "3  1.019880 -0.065087  ...  0.020638  1.018887  0.022129  1.019161  0.019855   \n",
      "4  1.023061 -0.065087  ...  0.020638  1.023884  0.022129  1.021753  0.019855   \n",
      "\n",
      "        252       253       254       255       256  \n",
      "0  1.018445  0.027825  1.019372  0.025792  1.021171  \n",
      "1  1.014788  0.027825  1.016499  0.025792  1.017849  \n",
      "2  1.021041  0.027825  1.022935  0.025792  1.022019  \n",
      "3  1.019916  0.027825  1.019602  0.025792  1.020735  \n",
      "4  1.019425  0.027825  1.018896  0.025792  1.016787  \n",
      "\n",
      "[5 rows x 257 columns] \n",
      "\n",
      "🔍 Outliers removed: 0\n",
      "✅ Rows after outlier removal: 0\n",
      "📌 Data After Removing Outliers:\n",
      " Empty DataFrame\n",
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 257 columns] \n",
      "\n",
      "🔍 Unwanted observations removed: 0\n",
      "✅ Rows after removing unwanted observations: 0\n",
      "📌 Data After Removing Unwanted Observations:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      "📌 Final number of rows: 0 (Rows Removed: 7352)\n",
      "✅ Cleaned data saved: Harus_Cleaned/train/Inertial Signals/total_acc_x_train.txt\n",
      "❌ Error processing Harus/train/Inertial Signals/body_acc_z_train.txt: Expected 155 fields in line 2, saw 176\n",
      "\n",
      "✅ Data cleaning completed for all Harus files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define directories\n",
    "input_dir = \"Harus\"  # Main dataset folder\n",
    "cleaned_dir = \"Harus_Cleaned\"  # Folder for cleaned files\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "# Function to clean each dataset\n",
    "def clean_dataset(file_path, save_path):\n",
    "    try:\n",
    "        # Read TXT file as DataFrame (assuming space-separated values)\n",
    "        df = pd.read_csv(file_path, sep=\" \", header=None, engine=\"python\")\n",
    "\n",
    "        print(f\"\\n🔹 Processing file: {os.path.basename(file_path)}\")\n",
    "\n",
    "        # Initial number of rows\n",
    "        initial_rows = df.shape[0]\n",
    "        print(f\"📌 Initial number of rows: {initial_rows}\")\n",
    "        print(\"📌 Original Data Sample:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 1️⃣ Handling Missing Values ###\n",
    "        missing_values_before = df.isnull().sum().sum()\n",
    "        print(f\"🔍 Missing values before: {missing_values_before}\")\n",
    "\n",
    "        # Fill numerical columns with median, categorical with mode\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'O':  # Categorical\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            else:  # Numerical\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "        missing_values_after = df.isnull().sum().sum()\n",
    "        print(f\"✅ Missing values after: {missing_values_after}\")\n",
    "        print(\"📌 Data After Handling Missing Values:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 2️⃣ Removing Redundant Values (Duplicates) ###\n",
    "        duplicate_rows = df.duplicated().sum()\n",
    "        print(f\"🔍 Duplicate rows found: {duplicate_rows}\")\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        after_duplicate_rows = df.shape[0]\n",
    "        print(f\"✅ Rows after duplicate removal: {after_duplicate_rows}\")\n",
    "        print(\"📌 Data After Removing Duplicates:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 3️⃣ Fixing Structural Errors (Trimming & Lowercasing) ###\n",
    "        df = df.applymap(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "        print(\"📌 Data After Fixing Structural Errors:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 4️⃣ Handling Outliers (IQR Method) ###\n",
    "        outliers_removed = 0\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            before_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
    "            outliers_removed += before_outliers\n",
    "            \n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "        after_outliers_rows = df.shape[0]\n",
    "        print(f\"🔍 Outliers removed: {outliers_removed}\")\n",
    "        print(f\"✅ Rows after outlier removal: {after_outliers_rows}\")\n",
    "        print(\"📌 Data After Removing Outliers:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        ### 5️⃣ Data Imputation (Re-check for missing values) ###\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                if df[col].dtype == 'O':\n",
    "                    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "                else:\n",
    "                    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "        ### 6️⃣ Removing Unwanted Observations ###\n",
    "        before_removal = df.shape[0]\n",
    "        df = df[~df.apply(lambda row: row.astype(str).str.contains(\"unknown|error\", case=False).any(), axis=1)]\n",
    "        removed_obs = before_removal - df.shape[0]\n",
    "        after_removal_rows = df.shape[0]\n",
    "        print(f\"🔍 Unwanted observations removed: {removed_obs}\")\n",
    "        print(f\"✅ Rows after removing unwanted observations: {after_removal_rows}\")\n",
    "        print(\"📌 Data After Removing Unwanted Observations:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "        # Final row count\n",
    "        final_rows = df.shape[0]\n",
    "        print(f\"📌 Final number of rows: {final_rows} (Rows Removed: {initial_rows - final_rows})\")\n",
    "\n",
    "        # Save cleaned data\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_csv(save_path, sep=\" \", index=False, header=False)\n",
    "\n",
    "        print(f\"✅ Cleaned data saved: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "\n",
    "# Traverse Harus directories and clean all text files\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            input_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(input_path, input_dir)  # Preserve directory structure\n",
    "            output_path = os.path.join(cleaned_dir, relative_path)\n",
    "            clean_dataset(input_path, output_path)\n",
    "\n",
    "print(\"\\n✅ Data cleaning completed for all Harus files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔢 Total rows in the dataset: 100000\n",
      "✅ Dataset saved successfully at /Users/jesicaanniebijju/Desktop/ADS/Merged_Dataset/mergeds_100k.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directories and file paths\n",
    "merged_dir = \"/Users/jesicaanniebijju/Desktop/ADS/Merged_Dataset\"  # Path to store the merged dataset\n",
    "output_file = \"mergeds_100k.csv\"  # Name for the final merged file\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(merged_dir):\n",
    "    os.makedirs(merged_dir)\n",
    "\n",
    "columns = ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'activity']\n",
    "merged_data = pd.DataFrame(np.random.randn(100000, len(columns)), columns=columns)  # Example with 100,000 rows\n",
    "\n",
    "# Check total rows in the dataset\n",
    "total_rows = len(merged_data)\n",
    "print(f\"\\n🔢 Total rows in the dataset: {total_rows}\")\n",
    "\n",
    "# Attempt to save the entire dataset to a single CSV\n",
    "try:\n",
    "    # Save the dataset in a single CSV file\n",
    "    merged_path = os.path.join(merged_dir, output_file)\n",
    "    merged_data.to_csv(merged_path, index=False)\n",
    "    print(f\"✅ Dataset saved successfully at {merged_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error while saving the dataset: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
